[druid] 2018-08-30 15:00:12,240 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 15:00:12,243 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 15:00:14,912 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 15:00:14,996 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 15:00:15,306 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 3
   [druid] 2018-08-30 15:00:15,976 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:3
   [druid] 2018-08-30 15:00:16,747 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local991453483_0001
   [druid] 2018-08-30 15:00:17,041 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Cleaning up the staging area file:/usr/local/hadoopdata/tmp/mapred/staging/Administrator991453483/.staging/job_local991453483_0001
   [druid] 2018-08-30 15:02:16,697 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 15:02:16,699 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 15:02:18,424 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 15:02:18,521 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 15:02:18,790 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 3
   [druid] 2018-08-30 15:02:19,307 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:3
   [druid] 2018-08-30 15:02:19,709 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2104659454_0001
   [druid] 2018-08-30 15:02:20,239 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 15:02:20,240 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2104659454_0001
   [druid] 2018-08-30 15:02:20,268 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 15:02:20,282 [Thread-17      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:02:20,285 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 15:02:20,428 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 15:02:20,430 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2104659454_0001_m_000000_0
   [druid] 2018-08-30 15:02:20,570 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:02:20,590 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:02:20,658 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1ba7d61
   [druid] 2018-08-30 15:02:20,668 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/subjectScore/chinese:0+88
   [druid] 2018-08-30 15:02:20,964 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 15:02:20,964 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 15:02:20,964 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 15:02:20,964 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 15:02:20,965 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 15:02:20,970 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 15:02:20,995 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 15:02:20,996 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 15:02:20,996 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 15:02:20,996 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 160; bufvoid = 104857600
   [druid] 2018-08-30 15:02:20,996 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600
   [druid] 2018-08-30 15:02:21,103 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 15:02:21,128 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2104659454_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:02:21,140 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 15:02:21,140 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2104659454_0001_m_000000_0' done.
   [druid] 2018-08-30 15:02:21,140 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2104659454_0001_m_000000_0
   [druid] 2018-08-30 15:02:21,140 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2104659454_0001_m_000001_0
   [druid] 2018-08-30 15:02:21,141 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:02:21,142 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:02:21,248 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@18288f2
   [druid] 2018-08-30 15:02:21,253 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/subjectScore/english:0+88
   [druid] 2018-08-30 15:02:21,304 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2104659454_0001 running in uber mode : false
   [druid] 2018-08-30 15:02:21,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 15:02:21,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 15:02:21,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 15:02:21,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 15:02:21,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 15:02:21,320 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 15:02:21,322 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 15:02:21,325 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 15:02:21,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 15:02:21,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 15:02:21,326 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 160; bufvoid = 104857600
   [druid] 2018-08-30 15:02:21,326 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600
   [druid] 2018-08-30 15:02:21,364 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 15:02:21,370 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2104659454_0001_m_000001_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:02:21,372 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 15:02:21,373 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2104659454_0001_m_000001_0' done.
   [druid] 2018-08-30 15:02:21,373 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2104659454_0001_m_000001_0
   [druid] 2018-08-30 15:02:21,373 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2104659454_0001_m_000002_0
   [druid] 2018-08-30 15:02:21,374 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:02:21,375 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:02:21,448 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9a90b9
   [druid] 2018-08-30 15:02:21,452 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/subjectScore/math:0+88
   [druid] 2018-08-30 15:02:21,519 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 15:02:21,519 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 15:02:21,519 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 15:02:21,520 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 15:02:21,520 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 15:02:21,521 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 15:02:21,524 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 15:02:21,525 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 15:02:21,525 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 15:02:21,525 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 130; bufvoid = 104857600
   [druid] 2018-08-30 15:02:21,525 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600
   [druid] 2018-08-30 15:02:21,547 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 15:02:21,559 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2104659454_0001_m_000002_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:02:21,562 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 15:02:21,563 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2104659454_0001_m_000002_0' done.
   [druid] 2018-08-30 15:02:21,563 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2104659454_0001_m_000002_0
   [druid] 2018-08-30 15:02:21,563 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 15:02:21,603 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 15:02:21,603 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2104659454_0001_r_000000_0
   [druid] 2018-08-30 15:02:21,621 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:02:21,621 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:02:21,729 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1cc5d5a
   [druid] 2018-08-30 15:02:21,746 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f45632
   [druid] 2018-08-30 15:02:21,787 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 15:02:21,795 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2104659454_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 15:02:21,863 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2104659454_0001_m_000000_0 decomp: 182 len: 186 to MEMORY
   [druid] 2018-08-30 15:02:21,889 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 182 bytes from map-output for attempt_local2104659454_0001_m_000000_0
   [druid] 2018-08-30 15:02:21,901 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 182, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->182
   [druid] 2018-08-30 15:02:21,907 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2104659454_0001_m_000001_0 decomp: 182 len: 186 to MEMORY
   [druid] 2018-08-30 15:02:21,908 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 182 bytes from map-output for attempt_local2104659454_0001_m_000001_0
   [druid] 2018-08-30 15:02:21,908 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 182, inMemoryMapOutputs.size() -> 2, commitMemory -> 182, usedMemory ->364
   [druid] 2018-08-30 15:02:21,914 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2104659454_0001_m_000002_0 decomp: 152 len: 156 to MEMORY
   [druid] 2018-08-30 15:02:21,915 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 152 bytes from map-output for attempt_local2104659454_0001_m_000002_0
   [druid] 2018-08-30 15:02:21,915 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 152, inMemoryMapOutputs.size() -> 3, commitMemory -> 364, usedMemory ->516
   [druid] 2018-08-30 15:02:21,915 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 15:02:21,918 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 3 / 3 copied.
   [druid] 2018-08-30 15:02:21,918 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 15:02:21,968 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 3 sorted segments
   [druid] 2018-08-30 15:02:21,969 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 3 segments left of total size: 489 bytes
   [druid] 2018-08-30 15:02:21,974 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 3 segments, 516 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 15:02:21,975 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 516 bytes from disk
   [druid] 2018-08-30 15:02:21,976 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 15:02:21,976 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 15:02:21,977 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 502 bytes
   [druid] 2018-08-30 15:02:21,978 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 3 / 3 copied.
   [druid] 2018-08-30 15:02:22,158 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 15:02:22,179 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2104659454_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:02:22,180 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 3 / 3 copied.
   [druid] 2018-08-30 15:02:22,180 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local2104659454_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 15:02:22,183 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local2104659454_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/subjectScoreout/_temporary/0/task_local2104659454_0001_r_000000
   [druid] 2018-08-30 15:02:22,188 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 15:02:22,188 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2104659454_0001_r_000000_0' done.
   [druid] 2018-08-30 15:02:22,189 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2104659454_0001_r_000000_0
   [druid] 2018-08-30 15:02:22,189 [Thread-17      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 15:02:22,325 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 15:02:23,326 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2104659454_0001 completed successfully
   [druid] 2018-08-30 15:02:23,341 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=5456
		FILE: Number of bytes written=1173818
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=30
		Map output records=30
		Map output bytes=450
		Map output materialized bytes=528
		Input split bytes=345
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=528
		Reduce input records=30
		Reduce output records=3
		Spilled Records=60
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=175
		Total committed heap usage (bytes)=743055360
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=264
	File Output Format Counters 
		Bytes Written=48
   [druid] 2018-08-30 15:07:55,227 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 15:07:55,229 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 15:07:56,942 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 15:07:57,025 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 15:07:57,302 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 3
   [druid] 2018-08-30 15:07:57,926 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:3
   [druid] 2018-08-30 15:07:58,303 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local92292453_0001
   [druid] 2018-08-30 15:07:58,412 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Cleaning up the staging area file:/usr/local/hadoopdata/tmp/mapred/staging/Administrator92292453/.staging/job_local92292453_0001
   [druid] 2018-08-30 15:38:31,870 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 15:38:31,873 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 15:38:33,677 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 15:38:33,760 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 15:38:33,829 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 15:38:34,309 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 15:38:34,738 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1281914888_0001
   [druid] 2018-08-30 15:38:35,181 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 15:38:35,182 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1281914888_0001
   [druid] 2018-08-30 15:38:35,184 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 15:38:35,189 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:35,191 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 15:38:35,243 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 15:38:35,244 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1281914888_0001_m_000000_0
   [druid] 2018-08-30 15:38:35,303 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:35,311 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:38:35,405 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@b7cdc6
   [druid] 2018-08-30 15:38:35,421 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 15:38:35,548 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 15:38:35,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 15:38:35,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 15:38:35,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 15:38:35,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 15:38:35,557 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 15:38:35,570 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 15:38:35,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 15:38:35,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 15:38:35,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 130; bufvoid = 104857600
   [druid] 2018-08-30 15:38:35,571 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 15:38:35,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 15:38:35,639 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1281914888_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:38:35,653 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 15:38:35,653 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1281914888_0001_m_000000_0' done.
   [druid] 2018-08-30 15:38:35,653 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1281914888_0001_m_000000_0
   [druid] 2018-08-30 15:38:35,653 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 15:38:35,655 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 15:38:35,656 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1281914888_0001_r_000000_0
   [druid] 2018-08-30 15:38:35,670 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:35,671 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:38:35,762 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@8f8391
   [druid] 2018-08-30 15:38:35,767 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@130e714
   [druid] 2018-08-30 15:38:35,791 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 15:38:35,794 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1281914888_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 15:38:35,860 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1281914888_0001_m_000000_0 decomp: 162 len: 166 to MEMORY
   [druid] 2018-08-30 15:38:35,872 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 162 bytes from map-output for attempt_local1281914888_0001_m_000000_0
   [druid] 2018-08-30 15:38:35,875 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162
   [druid] 2018-08-30 15:38:35,878 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 15:38:35,879 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:38:35,880 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 15:38:35,921 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 15:38:35,922 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 15:38:35,933 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 15:38:35,934 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 166 bytes from disk
   [druid] 2018-08-30 15:38:35,936 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 15:38:35,936 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 15:38:35,939 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 15:38:35,939 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:38:36,158 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 15:38:36,168 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:36,185 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1281914888_0001 running in uber mode : false
   [druid] 2018-08-30 15:38:36,187 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 15:38:36,341 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:36,543 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:36,703 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:38:36,880 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1281914888_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:38:36,882 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:38:36,882 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1281914888_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 15:38:36,953 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,954 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,954 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,954 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,955 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,955 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,956 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,956 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,957 [pool-3-thread-1] WARN  org.apache.hadoop.mapred.Task  {1} - Failure committing: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1281914888_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1281914888_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   [druid] 2018-08-30 15:38:36,960 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,960 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,961 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,961 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,962 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,962 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,963 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,963 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,964 [pool-3-thread-1] WARN  lib.output.FileOutputCommitter {1} - Could not delete file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1281914888_0001_r_000000_0
   [druid] 2018-08-30 15:38:36,964 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 15:38:36,988 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,989 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,989 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,990 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:38:36,990 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,991 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,991 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,991 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1281914888_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:38:36,992 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1281914888_0001
   java.lang.Exception: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1281914888_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1281914888_0001_r_000000
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1281914888_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1281914888_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 15:38:37,190 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1281914888_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 15:38:37,210 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=922
		FILE: Number of bytes written=597518
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=130
		Map output materialized bytes=166
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=166
		Reduce input records=15
		Reduce output records=0
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=56
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=8
   [druid] 2018-08-30 15:42:13,990 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 15:42:13,990 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 15:42:14,923 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 15:42:14,970 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 15:42:15,029 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 15:42:15,445 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 15:42:15,734 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1027368001_0001
   [druid] 2018-08-30 15:42:16,101 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 15:42:16,101 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1027368001_0001
   [druid] 2018-08-30 15:42:16,105 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 15:42:16,109 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:16,113 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 15:42:16,167 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 15:42:16,171 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1027368001_0001_m_000000_0
   [druid] 2018-08-30 15:42:16,198 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:16,202 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:42:16,296 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@f0d687
   [druid] 2018-08-30 15:42:16,304 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 15:42:16,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 15:42:16,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 15:42:16,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 15:42:16,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 15:42:16,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 15:42:16,402 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 15:42:16,417 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 15:42:16,417 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 15:42:16,417 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 15:42:16,417 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 130; bufvoid = 104857600
   [druid] 2018-08-30 15:42:16,417 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 15:42:16,445 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 15:42:16,452 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1027368001_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:42:16,464 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 15:42:16,464 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1027368001_0001_m_000000_0' done.
   [druid] 2018-08-30 15:42:16,464 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1027368001_0001_m_000000_0
   [druid] 2018-08-30 15:42:16,464 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 15:42:16,468 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 15:42:16,468 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1027368001_0001_r_000000_0
   [druid] 2018-08-30 15:42:16,476 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:16,480 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 15:42:16,550 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1ccc078
   [druid] 2018-08-30 15:42:16,558 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@f0d946
   [druid] 2018-08-30 15:42:16,574 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 15:42:16,577 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1027368001_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 15:42:16,628 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1027368001_0001_m_000000_0 decomp: 162 len: 166 to MEMORY
   [druid] 2018-08-30 15:42:16,632 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 162 bytes from map-output for attempt_local1027368001_0001_m_000000_0
   [druid] 2018-08-30 15:42:16,636 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162
   [druid] 2018-08-30 15:42:16,636 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 15:42:16,640 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:42:16,640 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 15:42:16,652 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 15:42:16,656 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 15:42:16,656 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 15:42:16,659 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 166 bytes from disk
   [druid] 2018-08-30 15:42:16,659 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 15:42:16,659 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 15:42:16,663 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 15:42:16,663 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:42:16,816 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 15:42:16,824 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:16,995 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:17,105 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1027368001_0001 running in uber mode : false
   [druid] 2018-08-30 15:42:17,105 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 15:42:17,117 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:17,261 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 15:42:17,398 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1027368001_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 15:42:17,398 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 15:42:17,402 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1027368001_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,445 [pool-3-thread-1] WARN  org.apache.hadoop.mapred.Task  {1} - Failure committing: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1027368001_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1027368001_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   [druid] 2018-08-30 15:42:17,449 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,449 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,449 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,449 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,453 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,453 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,453 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,453 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,453 [pool-3-thread-1] WARN  lib.output.FileOutputCommitter {1} - Could not delete file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1027368001_0001_r_000000_0
   [druid] 2018-08-30 15:42:17,453 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1027368001_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 15:42:17,468 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1027368001_0001
   java.lang.Exception: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1027368001_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1027368001_0001_r_000000
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1027368001_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1027368001_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 15:42:18,105 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1027368001_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 15:42:18,117 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=922
		FILE: Number of bytes written=597518
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=130
		Map output materialized bytes=166
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=166
		Reduce input records=15
		Reduce output records=0
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=33
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=8
   [druid] 2018-08-30 16:11:04,632 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:11:04,634 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:11:05,829 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:11:06,065 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:11:06,191 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:11:06,837 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:11:07,132 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local630448976_0001
   [druid] 2018-08-30 16:11:07,617 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:11:07,618 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local630448976_0001
   [druid] 2018-08-30 16:11:07,619 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:11:07,628 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:07,631 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:11:07,730 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:11:07,731 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local630448976_0001_m_000000_0
   [druid] 2018-08-30 16:11:07,775 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:07,782 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:11:07,844 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@34f625
   [druid] 2018-08-30 16:11:07,851 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:11:07,972 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:11:07,973 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:11:07,973 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:11:07,973 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:11:07,973 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:11:07,979 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:11:07,992 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:11:07,992 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:11:07,992 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:11:07,993 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 130; bufvoid = 104857600
   [druid] 2018-08-30 16:11:07,993 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:11:08,022 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:11:08,031 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local630448976_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:11:08,044 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:11:08,045 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local630448976_0001_m_000000_0' done.
   [druid] 2018-08-30 16:11:08,045 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local630448976_0001_m_000000_0
   [druid] 2018-08-30 16:11:08,045 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:11:08,049 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:11:08,049 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local630448976_0001_r_000000_0
   [druid] 2018-08-30 16:11:08,061 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:08,062 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:11:08,180 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1f0a8b0
   [druid] 2018-08-30 16:11:08,185 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ea8af
   [druid] 2018-08-30 16:11:08,213 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:11:08,217 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local630448976_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:11:08,349 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local630448976_0001_m_000000_0 decomp: 162 len: 166 to MEMORY
   [druid] 2018-08-30 16:11:08,358 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 162 bytes from map-output for attempt_local630448976_0001_m_000000_0
   [druid] 2018-08-30 16:11:08,360 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162
   [druid] 2018-08-30 16:11:08,362 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:11:08,363 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:11:08,364 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:11:08,385 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:11:08,385 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 16:11:08,389 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:11:08,390 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 166 bytes from disk
   [druid] 2018-08-30 16:11:08,392 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:11:08,393 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:11:08,394 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 16:11:08,396 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:11:08,601 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:11:08,612 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:08,623 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local630448976_0001 running in uber mode : false
   [druid] 2018-08-30 16:11:08,625 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 16:11:08,817 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:09,049 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:09,245 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:11:09,440 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local630448976_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:11:09,442 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:11:09,442 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local630448976_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:11:09,444 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local630448976_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local630448976_0001_r_000000
   [druid] 2018-08-30 16:11:09,445 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:11:09,445 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local630448976_0001_r_000000_0' done.
   [druid] 2018-08-30 16:11:09,446 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local630448976_0001_r_000000_0
   [druid] 2018-08-30 16:11:09,446 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:11:09,628 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 16:11:10,629 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local630448976_0001 completed successfully
   [druid] 2018-08-30 16:11:10,643 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=922
		FILE: Number of bytes written=594601
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=130
		Map output materialized bytes=166
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=166
		Reduce input records=15
		Reduce output records=0
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=48
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=8
   [druid] 2018-08-30 16:13:16,782 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:13:16,784 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:13:18,674 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:13:18,767 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:13:18,857 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:13:19,364 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:13:19,711 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1756380989_0001
   [druid] 2018-08-30 16:13:20,092 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:13:20,092 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1756380989_0001
   [druid] 2018-08-30 16:13:20,126 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:13:20,135 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:20,138 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:13:20,211 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:13:20,212 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1756380989_0001_m_000000_0
   [druid] 2018-08-30 16:13:20,242 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:20,250 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:13:20,323 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1f5a5b
   [druid] 2018-08-30 16:13:20,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:13:20,477 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:13:20,477 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:13:20,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:13:20,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:13:20,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:13:20,481 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:13:20,492 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:13:20,493 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:13:20,493 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:13:20,493 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 130; bufvoid = 104857600
   [druid] 2018-08-30 16:13:20,493 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:13:20,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:13:20,532 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1756380989_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:13:20,551 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:13:20,551 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1756380989_0001_m_000000_0' done.
   [druid] 2018-08-30 16:13:20,551 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1756380989_0001_m_000000_0
   [druid] 2018-08-30 16:13:20,551 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:13:20,554 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:13:20,555 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1756380989_0001_r_000000_0
   [druid] 2018-08-30 16:13:20,573 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:20,574 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:13:20,689 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@13e07bd
   [druid] 2018-08-30 16:13:20,695 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@193f119
   [druid] 2018-08-30 16:13:20,715 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:13:20,719 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1756380989_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:13:20,797 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1756380989_0001_m_000000_0 decomp: 162 len: 166 to MEMORY
   [druid] 2018-08-30 16:13:20,811 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 162 bytes from map-output for attempt_local1756380989_0001_m_000000_0
   [druid] 2018-08-30 16:13:20,814 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162
   [druid] 2018-08-30 16:13:20,821 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:13:20,822 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:13:20,822 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:13:20,846 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:13:20,847 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 16:13:20,850 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:13:20,854 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 166 bytes from disk
   [druid] 2018-08-30 16:13:20,858 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:13:20,859 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:13:20,860 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 158 bytes
   [druid] 2018-08-30 16:13:20,862 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:13:21,039 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:13:21,055 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:21,111 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1756380989_0001 running in uber mode : false
   [druid] 2018-08-30 16:13:21,113 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 16:13:21,263 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:21,478 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:21,687 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:13:21,885 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1756380989_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:13:21,887 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:13:21,887 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1756380989_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:13:21,949 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,950 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,950 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,951 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,952 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,952 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,953 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,953 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,954 [pool-3-thread-1] WARN  org.apache.hadoop.mapred.Task  {1} - Failure committing: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1756380989_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1756380989_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   [druid] 2018-08-30 16:13:21,957 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,959 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,959 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,960 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:21,960 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,961 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,961 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,962 [pool-3-thread-1] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:21,985 [pool-3-thread-1] WARN  lib.output.FileOutputCommitter {1} - Could not delete file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1756380989_0001_r_000000_0
   [druid] 2018-08-30 16:13:21,986 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:13:22,005 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.09-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:22,005 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.AB-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:22,006 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.az-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:22,007 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\.others-r-00000.crc]: it still exists.
   [druid] 2018-08-30 16:13:22,008 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\09-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:22,009 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\AB-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:22,009 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\az-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:22,009 [Thread-15      ] WARN  org.apache.hadoop.fs.FileUtil  {1} - Failed to delete file or dir [F:\Data\GP1809\wordcount\multiFilesout\_temporary\0\_temporary\attempt_local1756380989_0001_r_000000_0\others-r-00000]: it still exists.
   [druid] 2018-08-30 16:13:22,011 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1756380989_0001
   java.lang.Exception: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1756380989_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1756380989_0001_r_000000
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.io.IOException: Could not rename file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/_temporary/attempt_local1756380989_0001_r_000000_0 to file:/F:/Data/GP1809/wordcount/multiFilesout/_temporary/0/task_local1756380989_0001_r_000000
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:479)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:449)
	at org.apache.hadoop.mapred.Task.commit(Task.java:1200)
	at org.apache.hadoop.mapred.Task.done(Task.java:1062)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:397)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 16:13:22,115 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1756380989_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 16:13:22,129 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=922
		FILE: Number of bytes written=597518
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=130
		Map output materialized bytes=166
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=166
		Reduce input records=15
		Reduce output records=0
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=55
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=8
   [druid] 2018-08-30 16:36:49,903 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:36:49,905 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:36:51,411 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:36:51,542 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:36:51,941 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:36:52,494 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:36:52,900 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1800701187_0001
   [druid] 2018-08-30 16:36:53,337 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:36:53,338 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1800701187_0001
   [druid] 2018-08-30 16:36:53,341 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:36:53,352 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:36:53,353 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:36:53,455 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:36:53,456 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1800701187_0001_m_000000_0
   [druid] 2018-08-30 16:36:53,534 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:36:53,554 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:36:53,665 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@129271e
   [druid] 2018-08-30 16:36:53,689 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:36:53,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:36:53,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:36:53,816 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:36:53,816 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:36:53,816 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:36:53,821 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:36:53,832 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:36:53,888 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:36:53,892 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1800701187_0001
   java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.IntWritable
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.IntWritable
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1079)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:63)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:30)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 16:36:54,341 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1800701187_0001 running in uber mode : false
   [druid] 2018-08-30 16:36:54,343 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-08-30 16:36:54,348 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1800701187_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 16:36:54,356 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-08-30 16:37:41,503 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:37:41,506 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:37:43,267 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:37:43,355 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:37:43,459 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:37:43,983 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:37:44,335 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1404273797_0001
   [druid] 2018-08-30 16:37:44,698 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:37:44,700 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1404273797_0001
   [druid] 2018-08-30 16:37:44,700 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:37:44,708 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:37:44,710 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:37:44,776 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:37:44,776 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1404273797_0001_m_000000_0
   [druid] 2018-08-30 16:37:44,808 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:37:44,818 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:37:44,903 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1e2b2a6
   [druid] 2018-08-30 16:37:44,911 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:37:45,043 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:37:45,044 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:37:45,044 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:37:45,044 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:37:45,044 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:37:45,050 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:37:45,061 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:37:45,061 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:37:45,061 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:37:45,061 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 160; bufvoid = 104857600
   [druid] 2018-08-30 16:37:45,061 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:37:45,086 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:37:45,097 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1404273797_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:37:45,109 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:37:45,109 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1404273797_0001_m_000000_0' done.
   [druid] 2018-08-30 16:37:45,109 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1404273797_0001_m_000000_0
   [druid] 2018-08-30 16:37:45,110 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:37:45,114 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:37:45,115 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1404273797_0001_r_000000_0
   [druid] 2018-08-30 16:37:45,130 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:37:45,133 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:37:45,261 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1156187
   [druid] 2018-08-30 16:37:45,266 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7a4505
   [druid] 2018-08-30 16:37:45,290 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:37:45,295 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1404273797_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:37:45,369 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1404273797_0001_m_000000_0 decomp: 192 len: 196 to MEMORY
   [druid] 2018-08-30 16:37:45,377 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 192 bytes from map-output for attempt_local1404273797_0001_m_000000_0
   [druid] 2018-08-30 16:37:45,382 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 192, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->192
   [druid] 2018-08-30 16:37:45,384 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:37:45,385 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:37:45,385 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:37:45,407 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:37:45,408 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 185 bytes
   [druid] 2018-08-30 16:37:45,413 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 192 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:37:45,415 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 196 bytes from disk
   [druid] 2018-08-30 16:37:45,417 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:37:45,417 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:37:45,418 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 185 bytes
   [druid] 2018-08-30 16:37:45,419 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:37:45,643 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:37:45,655 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1404273797_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:37:45,657 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:37:45,657 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1404273797_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:37:45,660 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1404273797_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local1404273797_0001_r_000000
   [druid] 2018-08-30 16:37:45,661 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:37:45,661 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1404273797_0001_r_000000_0' done.
   [druid] 2018-08-30 16:37:45,662 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1404273797_0001_r_000000_0
   [druid] 2018-08-30 16:37:45,663 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:37:45,710 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1404273797_0001 running in uber mode : false
   [druid] 2018-08-30 16:37:45,713 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 16:37:46,715 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1404273797_0001 completed successfully
   [druid] 2018-08-30 16:37:46,725 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=982
		FILE: Number of bytes written=586866
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=160
		Map output materialized bytes=196
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=196
		Reduce input records=15
		Reduce output records=14
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=58
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=134
   [druid] 2018-08-30 16:39:25,511 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:39:25,514 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:39:27,277 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:39:27,358 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:39:27,459 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:39:28,078 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:39:28,464 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local730016501_0001
   [druid] 2018-08-30 16:39:28,839 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:39:28,840 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local730016501_0001
   [druid] 2018-08-30 16:39:28,842 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:39:28,849 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:39:28,851 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:39:28,925 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:39:28,925 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local730016501_0001_m_000000_0
   [druid] 2018-08-30 16:39:28,966 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:39:28,973 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:39:29,051 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@c73ac2
   [druid] 2018-08-30 16:39:29,067 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:39:29,189 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:39:29,189 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:39:29,189 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:39:29,190 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:39:29,190 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:39:29,195 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:39:29,205 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:39:29,232 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:39:29,235 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local730016501_0001
   java.lang.Exception: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to javax.xml.soap.Text
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to javax.xml.soap.Text
	at com.qfedu.bigdata.mr.day14.MyPartitionerDemo.getPartition(MyPartitionerDemo.java:24)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:716)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:63)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:30)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 16:39:29,842 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local730016501_0001 running in uber mode : false
   [druid] 2018-08-30 16:39:29,844 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-08-30 16:39:29,847 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local730016501_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 16:39:29,853 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-08-30 16:41:29,707 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:41:29,711 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:41:31,615 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:41:31,684 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:41:31,784 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:41:32,287 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:41:32,645 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1578225736_0001
   [druid] 2018-08-30 16:41:33,065 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:41:33,066 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1578225736_0001
   [druid] 2018-08-30 16:41:33,068 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:41:33,100 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:41:33,103 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:41:33,228 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:41:33,230 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1578225736_0001_m_000000_0
   [druid] 2018-08-30 16:41:33,264 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:41:33,271 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:41:33,346 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@10c266e
   [druid] 2018-08-30 16:41:33,355 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:41:33,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:41:33,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:41:33,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:41:33,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:41:33,478 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:41:33,483 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:41:33,492 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:41:33,553 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:41:33,555 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1578225736_0001
   java.lang.Exception: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to javax.xml.soap.Text
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to javax.xml.soap.Text
	at com.qfedu.bigdata.mr.day14.MyPartitionerDemo.getPartition(MyPartitionerDemo.java:24)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:716)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:60)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:27)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 16:41:34,069 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1578225736_0001 running in uber mode : false
   [druid] 2018-08-30 16:41:34,070 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-08-30 16:41:34,072 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1578225736_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 16:41:34,080 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-08-30 16:42:52,929 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:42:52,931 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:42:54,607 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:42:54,689 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:42:54,787 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:42:55,315 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:42:55,761 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1978402179_0001
   [druid] 2018-08-30 16:42:56,182 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:42:56,183 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1978402179_0001
   [druid] 2018-08-30 16:42:56,184 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:42:56,188 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:56,190 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:42:56,269 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:42:56,269 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:56,296 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:56,306 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:42:56,371 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@34ca1a
   [druid] 2018-08-30 16:42:56,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:42:56,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:42:56,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:42:56,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:42:56,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:42:56,522 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:42:56,542 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:42:56,555 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:42:56,556 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:42:56,556 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:42:56,556 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 160; bufvoid = 104857600
   [druid] 2018-08-30 16:42:56,556 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:42:56,597 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:42:56,611 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1978402179_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:42:56,622 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:42:56,622 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1978402179_0001_m_000000_0' done.
   [druid] 2018-08-30 16:42:56,622 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:56,622 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:42:56,627 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:42:56,627 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1978402179_0001_r_000000_0
   [druid] 2018-08-30 16:42:56,643 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:56,645 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:42:56,749 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@55acbc
   [druid] 2018-08-30 16:42:56,755 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4d5e50
   [druid] 2018-08-30 16:42:56,787 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:42:56,794 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1978402179_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:42:56,878 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1978402179_0001_m_000000_0 decomp: 50 len: 54 to MEMORY
   [druid] 2018-08-30 16:42:56,885 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 50 bytes from map-output for attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:56,887 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->50
   [druid] 2018-08-30 16:42:56,889 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:42:56,890 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:56,891 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:42:56,946 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:56,946 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 43 bytes
   [druid] 2018-08-30 16:42:56,949 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 50 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:42:56,950 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 54 bytes from disk
   [druid] 2018-08-30 16:42:56,952 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:42:56,952 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:56,953 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 43 bytes
   [druid] 2018-08-30 16:42:56,954 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,184 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1978402179_0001 running in uber mode : false
   [druid] 2018-08-30 16:42:57,187 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 16:42:57,190 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:42:57,200 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1978402179_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:42:57,202 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,202 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1978402179_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:42:57,204 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1978402179_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local1978402179_0001_r_000000
   [druid] 2018-08-30 16:42:57,205 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:42:57,205 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1978402179_0001_r_000000_0' done.
   [druid] 2018-08-30 16:42:57,205 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1978402179_0001_r_000000_0
   [druid] 2018-08-30 16:42:57,205 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1978402179_0001_r_000001_0
   [druid] 2018-08-30 16:42:57,207 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:57,207 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:42:57,298 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@126c1c6
   [druid] 2018-08-30 16:42:57,298 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@17bd8d0
   [druid] 2018-08-30 16:42:57,300 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:42:57,305 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1978402179_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:42:57,311 [localfetcher#2 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#2 about to shuffle output of map attempt_local1978402179_0001_m_000000_0 decomp: 82 len: 86 to MEMORY
   [druid] 2018-08-30 16:42:57,312 [localfetcher#2 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 82 bytes from map-output for attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:57,312 [localfetcher#2 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 82, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82
   [druid] 2018-08-30 16:42:57,313 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:42:57,314 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,314 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:42:57,326 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:57,326 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 68 bytes
   [druid] 2018-08-30 16:42:57,336 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 82 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:42:57,337 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 86 bytes from disk
   [druid] 2018-08-30 16:42:57,338 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:42:57,338 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:57,339 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 68 bytes
   [druid] 2018-08-30 16:42:57,339 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,548 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1978402179_0001_r_000001_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:42:57,550 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,550 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1978402179_0001_r_000001_0 is allowed to commit now
   [druid] 2018-08-30 16:42:57,553 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1978402179_0001_r_000001_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local1978402179_0001_r_000001
   [druid] 2018-08-30 16:42:57,554 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:42:57,554 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1978402179_0001_r_000001_0' done.
   [druid] 2018-08-30 16:42:57,554 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1978402179_0001_r_000001_0
   [druid] 2018-08-30 16:42:57,554 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1978402179_0001_r_000002_0
   [druid] 2018-08-30 16:42:57,556 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:57,556 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:42:57,651 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1de56e2
   [druid] 2018-08-30 16:42:57,651 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a323d
   [druid] 2018-08-30 16:42:57,654 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:42:57,658 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1978402179_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:42:57,663 [localfetcher#3 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#3 about to shuffle output of map attempt_local1978402179_0001_m_000000_0 decomp: 35 len: 39 to MEMORY
   [druid] 2018-08-30 16:42:57,663 [localfetcher#3 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 35 bytes from map-output for attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:57,664 [localfetcher#3 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 35, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->35
   [druid] 2018-08-30 16:42:57,665 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:42:57,665 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,665 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:42:57,701 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:57,701 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 27 bytes
   [druid] 2018-08-30 16:42:57,703 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 35 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:42:57,704 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 39 bytes from disk
   [druid] 2018-08-30 16:42:57,704 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:42:57,704 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:57,705 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 27 bytes
   [druid] 2018-08-30 16:42:57,706 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,903 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1978402179_0001_r_000002_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:42:57,906 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:57,906 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1978402179_0001_r_000002_0 is allowed to commit now
   [druid] 2018-08-30 16:42:57,911 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1978402179_0001_r_000002_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local1978402179_0001_r_000002
   [druid] 2018-08-30 16:42:57,912 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:42:57,913 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1978402179_0001_r_000002_0' done.
   [druid] 2018-08-30 16:42:57,913 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1978402179_0001_r_000002_0
   [druid] 2018-08-30 16:42:57,913 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1978402179_0001_r_000003_0
   [druid] 2018-08-30 16:42:57,914 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:42:57,915 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:42:58,002 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9198ea
   [druid] 2018-08-30 16:42:58,002 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@ce3c61
   [druid] 2018-08-30 16:42:58,004 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:42:58,007 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1978402179_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:42:58,011 [localfetcher#4 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#4 about to shuffle output of map attempt_local1978402179_0001_m_000000_0 decomp: 31 len: 35 to MEMORY
   [druid] 2018-08-30 16:42:58,012 [localfetcher#4 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31 bytes from map-output for attempt_local1978402179_0001_m_000000_0
   [druid] 2018-08-30 16:42:58,012 [localfetcher#4 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31
   [druid] 2018-08-30 16:42:58,017 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:42:58,018 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:58,018 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:42:58,052 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:58,053 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 20 bytes
   [druid] 2018-08-30 16:42:58,055 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:42:58,056 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 35 bytes from disk
   [druid] 2018-08-30 16:42:58,056 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:42:58,056 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:42:58,058 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 20 bytes
   [druid] 2018-08-30 16:42:58,060 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:58,196 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 75%
   [druid] 2018-08-30 16:42:58,283 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1978402179_0001_r_000003_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:42:58,286 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:42:58,287 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1978402179_0001_r_000003_0 is allowed to commit now
   [druid] 2018-08-30 16:42:58,300 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1978402179_0001_r_000003_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local1978402179_0001_r_000003
   [druid] 2018-08-30 16:42:58,301 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:42:58,301 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1978402179_0001_r_000003_0' done.
   [druid] 2018-08-30 16:42:58,301 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1978402179_0001_r_000003_0
   [druid] 2018-08-30 16:42:58,301 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:42:59,198 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 16:42:59,199 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1978402179_0001 completed successfully
   [druid] 2018-08-30 16:42:59,241 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=4541
		FILE: Number of bytes written=1467802
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=160
		Map output materialized bytes=214
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=214
		Reduce input records=15
		Reduce output records=14
		Spilled Records=30
		Shuffled Maps =4
		Failed Shuffles=0
		Merged Map outputs=4
		GC time elapsed (ms)=66
		Total committed heap usage (bytes)=605900800
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=170
   [druid] 2018-08-30 16:44:04,958 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:44:04,963 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:44:06,291 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:44:06,342 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:44:06,511 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:44:07,239 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:44:07,649 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2146405611_0001
   [druid] 2018-08-30 16:44:08,063 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:44:08,065 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2146405611_0001
   [druid] 2018-08-30 16:44:08,067 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:44:08,076 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:08,079 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:44:08,151 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:44:08,152 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2146405611_0001_m_000000_0
   [druid] 2018-08-30 16:44:08,199 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:08,211 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:44:08,279 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3170cc
   [druid] 2018-08-30 16:44:08,287 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:44:08,418 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:44:08,418 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:44:08,418 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:44:08,418 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:44:08,418 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:44:08,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:44:08,437 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:44:08,438 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:44:08,438 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:44:08,440 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 160; bufvoid = 104857600
   [druid] 2018-08-30 16:44:08,440 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:44:08,470 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:44:08,484 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2146405611_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:44:08,501 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:44:08,502 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2146405611_0001_m_000000_0' done.
   [druid] 2018-08-30 16:44:08,502 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2146405611_0001_m_000000_0
   [druid] 2018-08-30 16:44:08,503 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:44:08,507 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:44:08,507 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2146405611_0001_r_000000_0
   [druid] 2018-08-30 16:44:08,522 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:08,526 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:44:08,642 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@64e8ad
   [druid] 2018-08-30 16:44:08,646 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e05ace
   [druid] 2018-08-30 16:44:08,670 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:44:08,674 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2146405611_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:44:08,761 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2146405611_0001_m_000000_0 decomp: 83 len: 87 to MEMORY
   [druid] 2018-08-30 16:44:08,768 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 83 bytes from map-output for attempt_local2146405611_0001_m_000000_0
   [druid] 2018-08-30 16:44:08,770 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 83, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->83
   [druid] 2018-08-30 16:44:08,774 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:44:08,775 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:08,775 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:44:08,800 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:44:08,800 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 76 bytes
   [druid] 2018-08-30 16:44:08,803 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 83 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:44:08,804 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 87 bytes from disk
   [druid] 2018-08-30 16:44:08,806 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:44:08,807 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:44:08,808 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 76 bytes
   [druid] 2018-08-30 16:44:08,808 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:09,003 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:44:09,035 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2146405611_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:44:09,036 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:09,036 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local2146405611_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:44:09,040 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local2146405611_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local2146405611_0001_r_000000
   [druid] 2018-08-30 16:44:09,042 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:44:09,043 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2146405611_0001_r_000000_0' done.
   [druid] 2018-08-30 16:44:09,043 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2146405611_0001_r_000000_0
   [druid] 2018-08-30 16:44:09,043 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2146405611_0001_r_000001_0
   [druid] 2018-08-30 16:44:09,044 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:09,045 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:44:09,082 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2146405611_0001 running in uber mode : false
   [druid] 2018-08-30 16:44:09,094 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 16:44:09,155 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@204bcc
   [druid] 2018-08-30 16:44:09,155 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@504a73
   [druid] 2018-08-30 16:44:09,156 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:44:09,163 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2146405611_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:44:09,168 [localfetcher#2 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#2 about to shuffle output of map attempt_local2146405611_0001_m_000000_0 decomp: 111 len: 115 to MEMORY
   [druid] 2018-08-30 16:44:09,170 [localfetcher#2 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 111 bytes from map-output for attempt_local2146405611_0001_m_000000_0
   [druid] 2018-08-30 16:44:09,170 [localfetcher#2 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 111, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->111
   [druid] 2018-08-30 16:44:09,172 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:44:09,173 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:09,173 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:44:09,187 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:44:09,187 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 100 bytes
   [druid] 2018-08-30 16:44:09,191 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 111 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:44:09,192 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 115 bytes from disk
   [druid] 2018-08-30 16:44:09,192 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:44:09,192 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:44:09,193 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 100 bytes
   [druid] 2018-08-30 16:44:09,193 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:09,424 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2146405611_0001_r_000001_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:44:09,432 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:44:09,433 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local2146405611_0001_r_000001_0 is allowed to commit now
   [druid] 2018-08-30 16:44:09,436 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local2146405611_0001_r_000001_0' to file:/F:/Data/GP1809/wordcount/multiFilesout1/_temporary/0/task_local2146405611_0001_r_000001
   [druid] 2018-08-30 16:44:09,437 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:44:09,438 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2146405611_0001_r_000001_0' done.
   [druid] 2018-08-30 16:44:09,438 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2146405611_0001_r_000001_0
   [druid] 2018-08-30 16:44:09,438 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:44:10,097 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2146405611_0001 completed successfully
   [druid] 2018-08-30 16:44:10,113 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=1813
		FILE: Number of bytes written=880376
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=160
		Map output materialized bytes=202
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=202
		Reduce input records=15
		Reduce output records=14
		Spilled Records=30
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=60
		Total committed heap usage (bytes)=363540480
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=146
   [druid] 2018-08-30 16:44:48,826 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:44:48,828 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:44:50,606 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:44:50,684 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:44:50,796 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:44:51,368 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:44:51,788 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1892906697_0001
   [druid] 2018-08-30 16:44:52,173 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:44:52,174 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1892906697_0001
   [druid] 2018-08-30 16:44:52,177 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:44:52,186 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:52,188 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:44:52,264 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:44:52,264 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1892906697_0001_m_000000_0
   [druid] 2018-08-30 16:44:52,306 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:44:52,315 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:44:52,404 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@102ead5
   [druid] 2018-08-30 16:44:52,415 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:44:52,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:44:52,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:44:52,550 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:44:52,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:44:52,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:44:52,558 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:44:52,569 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:44:52,569 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:44:52,569 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 20; bufvoid = 104857600
   [druid] 2018-08-30 16:44:52,569 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
   [druid] 2018-08-30 16:44:52,601 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:44:52,614 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:44:52,618 [Thread-15      ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1892906697_0001
   java.lang.Exception: java.io.IOException: Illegal partition for Hello (2)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: Illegal partition for Hello (2)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1082)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:60)
	at com.qfedu.bigdata.mr.day14.partitionerDemo$MyMapper.map(partitionerDemo.java:27)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[druid] 2018-08-30 16:44:53,177 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1892906697_0001 running in uber mode : false
   [druid] 2018-08-30 16:44:53,178 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-08-30 16:44:53,181 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1892906697_0001 failed with state FAILED due to: NA
   [druid] 2018-08-30 16:44:53,185 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-08-30 16:50:57,607 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-08-30 16:50:57,611 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-08-30 16:50:59,149 [main           ] WARN  .mapreduce.JobResourceUploader {1} - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
   [druid] 2018-08-30 16:50:59,219 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-08-30 16:50:59,300 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-08-30 16:50:59,797 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-08-30 16:51:00,197 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local992212807_0001
   [druid] 2018-08-30 16:51:00,624 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-08-30 16:51:00,626 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local992212807_0001
   [druid] 2018-08-30 16:51:00,628 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-08-30 16:51:00,635 [Thread-15      ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:51:00,637 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-08-30 16:51:00,784 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-08-30 16:51:00,784 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local992212807_0001_m_000000_0
   [druid] 2018-08-30 16:51:00,840 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:51:00,848 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:51:00,947 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1859d8c
   [druid] 2018-08-30 16:51:00,956 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: file:/F:/Data/GP1809/wordcount/multiFiles/multiFile:0+109
   [druid] 2018-08-30 16:51:01,073 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-08-30 16:51:01,073 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-08-30 16:51:01,073 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-08-30 16:51:01,074 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-08-30 16:51:01,074 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-08-30 16:51:01,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-08-30 16:51:01,100 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-08-30 16:51:01,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-08-30 16:51:01,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-08-30 16:51:01,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220; bufvoid = 104857600
   [druid] 2018-08-30 16:51:01,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600
   [druid] 2018-08-30 16:51:01,182 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-08-30 16:51:01,196 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local992212807_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:51:01,217 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-08-30 16:51:01,217 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local992212807_0001_m_000000_0' done.
   [druid] 2018-08-30 16:51:01,217 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local992212807_0001_m_000000_0
   [druid] 2018-08-30 16:51:01,217 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-08-30 16:51:01,221 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-08-30 16:51:01,226 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local992212807_0001_r_000000_0
   [druid] 2018-08-30 16:51:01,239 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-08-30 16:51:01,240 [pool-3-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-08-30 16:51:01,363 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@13ff9d4
   [druid] 2018-08-30 16:51:01,372 [pool-3-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e5305e
   [druid] 2018-08-30 16:51:01,396 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=181665792, maxSingleShuffleLimit=45416448, mergeThreshold=119899424, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-08-30 16:51:01,409 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local992212807_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-08-30 16:51:01,473 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local992212807_0001_m_000000_0 decomp: 236 len: 240 to MEMORY
   [druid] 2018-08-30 16:51:01,477 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 236 bytes from map-output for attempt_local992212807_0001_m_000000_0
   [druid] 2018-08-30 16:51:01,478 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 236, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->236
   [druid] 2018-08-30 16:51:01,479 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-08-30 16:51:01,480 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:51:01,481 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-08-30 16:51:01,505 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:51:01,506 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 229 bytes
   [druid] 2018-08-30 16:51:01,507 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 236 bytes to disk to satisfy reduce memory limit
   [druid] 2018-08-30 16:51:01,508 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 240 bytes from disk
   [druid] 2018-08-30 16:51:01,509 [pool-3-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-08-30 16:51:01,510 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-08-30 16:51:01,510 [pool-3-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 229 bytes
   [druid] 2018-08-30 16:51:01,511 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:51:01,632 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local992212807_0001 running in uber mode : false
   [druid] 2018-08-30 16:51:01,634 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-08-30 16:51:01,747 [pool-3-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-08-30 16:51:01,751 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local992212807_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-08-30 16:51:01,753 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-08-30 16:51:01,753 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local992212807_0001_r_000000_0 is allowed to commit now
   [druid] 2018-08-30 16:51:01,756 [pool-3-thread-1] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local992212807_0001_r_000000_0' to file:/F:/Data/GP1809/wordcount/multiFilesout2/_temporary/0/task_local992212807_0001_r_000000
   [druid] 2018-08-30 16:51:01,756 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-08-30 16:51:01,757 [pool-3-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local992212807_0001_r_000000_0' done.
   [druid] 2018-08-30 16:51:01,757 [pool-3-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local992212807_0001_r_000000_0
   [druid] 2018-08-30 16:51:01,757 [Thread-15      ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-08-30 16:51:02,637 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-08-30 16:51:02,637 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local992212807_0001 completed successfully
   [druid] 2018-08-30 16:51:02,653 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 30
	File System Counters
		FILE: Number of bytes read=1070
		FILE: Number of bytes written=583930
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=15
		Map output bytes=220
		Map output materialized bytes=240
		Input split bytes=116
		Combine input records=15
		Combine output records=14
		Reduce input groups=14
		Reduce shuffle bytes=240
		Reduce input records=14
		Reduce output records=14
		Spilled Records=28
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=44
		Total committed heap usage (bytes)=242360320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=109
	File Output Format Counters 
		Bytes Written=134
   